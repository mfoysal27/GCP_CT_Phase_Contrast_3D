{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b711144-b9bf-47b7-9b72-a5dd9c68ee48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#container scaffolding\n",
    "#updating to write all under capstone_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a54e76-ddd1-4eb5-aa30-941fc7ac890f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudml-hypertune in /home/jupyter/.local/lib/python3.7/site-packages (0.1.0.dev6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install cloudml-hypertune --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ec98964e-4925-40e3-a9fc-b119f2aa0247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hypertune \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2935a968-119f-4db8-8a56-e8f55bccd294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = 'rutu-bucket'\n",
    "REGION = \"us-central1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "68e17469-f287-42d9-88df-b13b387b2fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rutu-bucket\n"
     ]
    }
   ],
   "source": [
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "151d9138-8370-4cdf-81d0-2867ade9f3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "69666b00-8c2c-4f22-95dc-1dbe9594e326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rutu-bucket\n"
     ]
    }
   ],
   "source": [
    "!echo $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3d06c794-e261-4b66-be96-ed643cd4393d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rutu-bucket/ :\n",
      "\tLocation type:\t\t\tregion\n",
      "\tLocation constraint:\t\tUS-CENTRAL1\n",
      "us-central1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi\n",
    "gsutil ls -Lb gs://$BUCKET | grep \"gs://\\|Location\"\n",
    "echo $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3b7e59d7-35ce-47dd-8e97-af6b0de26828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project ${PROJECT}\n",
    "gcloud config set ai/region ${REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f3ead1e-290c-4746-8fca-ef29fbdef5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4f2a157d-9529-4120-804b-5352182c29d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer/__init__.py  trainer/model.py  trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "!ls -R trainer/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7cdec662-f724-4a1e-a191-9240793b8c66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/rutuja_april5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Prints the current working directory\n",
    "#os.chdir('/home/jupyter/j-local-capstone')\n",
    "#print(os.getcwd())  # Prints the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e59a47d7-5ea2-4f46-b3e0-d198428a8246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    " \n",
    "\n",
    "# getting the name of the directory\n",
    "# where the this file is present.\n",
    "current = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "parent = os.path.dirname(current)\n",
    "\n",
    "# adding the parent directory to\n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "#import model\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data, e.g., gs://rutu-bucket/train_3d.csv\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data, e.g., gs://rutu-bucket/valid_3d.csv\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        default=os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--batch_size\",\n",
    "    #     help=\"Number of examples to compute gradient over.\",\n",
    "    #     type=int,\n",
    "    #     default=512\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--nnsize\",\n",
    "    #     help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "    #     default=\"64 64\"\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=40\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--train_examples\",\n",
    "    #     help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "    #     If this is more than actual # of examples available, it cycles through\n",
    "    #     them. So specifying 1000 here when you have only 100k examples makes\n",
    "    #     this 10 epochs.\"\"\",\n",
    "    #     type=int,\n",
    "    #     default=5000\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--eval_steps\",\n",
    "    #     help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "    #     to None, which means to evaluate until input_fn raises an end-of-input\n",
    "    #     exception\"\"\",\n",
    "    #     type=int,\n",
    "    #     default=None\n",
    "    # )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dropout_rate\",\n",
    "        help=\"\"\"dropout_rate\"\"\",\n",
    "        type=float,\n",
    "        default=0.2)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--l2_regularization_lambda\",\n",
    "        help=\"\"\"l2_regularization_lambda\"\"\",\n",
    "        type=float,\n",
    "        default=0.1)\n",
    "    parser.add_argument(\"--hptune\", action='store_true')  # if hptune is not set, it is False in default\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11667f43-4ab4-4032-abe9-73f7bbd31f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import datetime\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import hypertune \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv3D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling3D,\n",
    "    Softmax\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=8)\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "\n",
    "labels_to_numeric = {\n",
    "    \"Arterial\": 0,\n",
    "    \"Late\": 1,\n",
    "    \"Non-Contrast\": 2,\n",
    "    \"Venous\": 3\n",
    "}\n",
    "\n",
    "numeric_to_labels = {\n",
    "    0:   \"Arterial\",\n",
    "    1:   \"Late\",    \n",
    "    2:   \"Non-Contrast\",    \n",
    "    3:  \"Venous\"\n",
    "}\n",
    "\n",
    "\n",
    "def reshape_and_normalize(images):\n",
    "    \n",
    "    ### START CODE HEREA\n",
    "\n",
    "    # Reshape the images to add an extra dimension\n",
    "    images = images.reshape((images.shape[0], images.shape[1], images.shape[2], images.shape[3],1))\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    max_value = np.max(images)\n",
    "    images = images/max_value\n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    return images,max_value# Reload the images in case you run this cell multiple times\n",
    "\n",
    "\n",
    "def load_and_format_data_from_gcs(sample_dir):\n",
    "    # sample_dir=\"gs://capstone-datasets/train_3d.csv\"\n",
    "    file_list = file_io.read_file_to_string(sample_dir).split(\"\\n\")\n",
    "    images = np.array([np.load(BytesIO(file_io.read_file_to_string(file, binary_mode=True)))\n",
    "                      for file in file_list if file])\n",
    "    labels = np.array([os.path.basename(file).split(\"_\")[4] for file in file_list if file])\n",
    "    labels = np.array([labels_to_numeric[label] for label in labels])\n",
    "    one_hots = to_categorical(labels)\n",
    "\n",
    "    images_tranformed, max_value = reshape_and_normalize(images)\n",
    "    return images_tranformed, one_hots\n",
    "\n",
    "#training_sample_dir = \"gs://rutu-bucket/train_3d.csv\"\n",
    "#training_images, one_hots = load_and_format_data_from_gcs(training_sample_dir)\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Define the method that checks the accuracy at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('val_accuracy') > 0.82:\n",
    "            print(\"Reached 82% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "            \n",
    "            \n",
    "#callbacks = myCallback()\n",
    "def convolutional_model(training_images_shape=(32, 128, 128, 1)):\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Conv3D(64, 3, activation='relu',input_shape=training_images_shape),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=2, strides=2, padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=2, strides=2, padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(128, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=2, strides=2, padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(256, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=2, strides=2, padding='same'),\n",
    "        \n",
    "        tf.keras.layers.GlobalAveragePooling3D(),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Softmax()      \n",
    "    ])\n",
    "    ### END CODE HERE\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "# Instantiate the HyperTune reporting object\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='val_loss',\n",
    "            metric_value=logs['val_accuracy'],\n",
    "            global_step=epoch)\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "\n",
    "    training_images, one_hots_train = load_and_format_data_from_gcs(args[\"train_data_path\"])\n",
    "    print(f\"Maximum pixel value after normalization: {np.max(training_images)}\\n\")\n",
    "    print(f\"Shape of training set after reshaping: {training_images.shape}\\n\")\n",
    "    print(f\"Shape of one image after reshaping: {training_images[0].shape}\")\n",
    "\n",
    "    valid_images, one_hots_valid = load_and_format_data_from_gcs(args[\"eval_data_path\"])\n",
    "    print(f\"Maximum pixel value after normalization: {np.max(valid_images)}\\n\")\n",
    "    print(f\"Shape of training set after reshaping: {valid_images.shape}\\n\")\n",
    "    print(f\"Shape of one image after reshaping: {valid_images[0].shape}\")\n",
    "\n",
    "    model = convolutional_model(training_images_shape=training_images.shape[1:])\n",
    "    print(\"Here is our model so far:\\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    #checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/phase_contrast\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=AIP_MODEL_DIR, verbose=1, save_weights_only=True)\n",
    "\n",
    "    callbacks = myCallback()\n",
    "\n",
    "    history = model.fit(x=training_images, \n",
    "                    y=one_hots_train,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(valid_images,one_hots_valid), \n",
    "                    epochs=40, \n",
    "                    callbacks=[callbacks, cp_callback, HPTCallback()])\n",
    "\n",
    "    \n",
    "    hptune = args[\"hptune\"]\n",
    "    history_history = history.history\n",
    "    print(f\"history_history: {pp.pformat(history_history)}\")\n",
    "    final_val_accuracy = history_history.get(\"val_accuracy\")[-1]\n",
    "    print(f\"Final Validation accuracy: {final_val_accuracy}\")\n",
    "    if hptune:\n",
    "        # Log it with hypertune\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=\"final_val_accuracy\", metric_value=final_val_accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model #updated this j\n",
    "    if not hptune:\n",
    "        \n",
    "        tf.saved_model.save(\n",
    "            obj=model, export_dir=AIP_MODEL_DIR)  # with default serving function\n",
    "\n",
    "    print(\"Exported trained model to {}\".format(AIP_MODEL_DIR))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #EXPORT_PATH = os.path.join(args[\"AIP_MODEL_DIR\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    \n",
    "    #tf.saved_model.save(obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    #tf.saved_model.save(obj=model, export_dir=AIP_MODEL_DIR)  # with default serving function\n",
    "\n",
    "    #print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "747c433d-ed36-4938-8928-0fdbcc060529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680874989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "START=$(date +%s);\n",
    "echo $START\n",
    "#sleep 1; # Your stuff\n",
    "\n",
    "#export PYTHONPATH=${PYTHONPATH}:${PWD}/capstone_j\n",
    "echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d61143e7-a7c2-4ecb-abb9-c6775fa80ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!ls -l -R *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54d61305-1e1e-406d-a034-9b6b1251a5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680804240\n",
      "1680804253\n",
      "0:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/rutuja_april5/trainer/task.py\", line 20, in <module>\n",
      "    from trainer import model\n",
      "  File \"/home/jupyter/rutuja_april5/trainer/model.py\", line 23, in <module>\n",
      "    AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
      "  File \"/opt/conda/lib/python3.7/os.py\", line 681, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'AIP_MODEL_DIR'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "START=$(date +%s);\n",
    "echo $START\n",
    "#sleep 1; # Your stuff\n",
    "\n",
    "BUCKET=rutu-bucket\n",
    "OUTDIR=capstone1_trained_j_3_1\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "#PYTHONPATH='/home/jupyter/j-local-capstone/capstone_j/'\n",
    "python3 -m trainer.task \\\n",
    "    --train_data_path=gs://${BUCKET}/train_3d.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/valid_3d.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=1 \n",
    "\n",
    "END=$(date +%s);\n",
    "echo $END\n",
    "echo $((END-START)) | awk '{print int($1/60)\":\"int($1%60)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9dbf16f5-c7c8-49a7-9383-9b1e00f5b2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='rutu_capstone_trainer_5_1',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='CT contrast model training application2.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2e92cc5b-7183-4fb8-a52e-5d116205564a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing rutu_capstone_trainer_5_1.egg-info/PKG-INFO\n",
      "writing dependency_links to rutu_capstone_trainer_5_1.egg-info/dependency_links.txt\n",
      "writing top-level names to rutu_capstone_trainer_5_1.egg-info/top_level.txt\n",
      "reading manifest file 'rutu_capstone_trainer_5_1.egg-info/SOURCES.txt'\n",
      "writing manifest file 'rutu_capstone_trainer_5_1.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating rutu_capstone_trainer_5_1-0.1\n",
      "creating rutu_capstone_trainer_5_1-0.1/rutu_capstone_trainer_5_1.egg-info\n",
      "creating rutu_capstone_trainer_5_1-0.1/trainer\n",
      "copying files to rutu_capstone_trainer_5_1-0.1...\n",
      "copying setup.py -> rutu_capstone_trainer_5_1-0.1\n",
      "copying rutu_capstone_trainer_5_1.egg-info/PKG-INFO -> rutu_capstone_trainer_5_1-0.1/rutu_capstone_trainer_5_1.egg-info\n",
      "copying rutu_capstone_trainer_5_1.egg-info/SOURCES.txt -> rutu_capstone_trainer_5_1-0.1/rutu_capstone_trainer_5_1.egg-info\n",
      "copying rutu_capstone_trainer_5_1.egg-info/dependency_links.txt -> rutu_capstone_trainer_5_1-0.1/rutu_capstone_trainer_5_1.egg-info\n",
      "copying rutu_capstone_trainer_5_1.egg-info/top_level.txt -> rutu_capstone_trainer_5_1-0.1/rutu_capstone_trainer_5_1.egg-info\n",
      "copying trainer/__init__.py -> rutu_capstone_trainer_5_1-0.1/trainer\n",
      "copying trainer/model.py -> rutu_capstone_trainer_5_1-0.1/trainer\n",
      "copying trainer/task.py -> rutu_capstone_trainer_5_1-0.1/trainer\n",
      "Writing rutu_capstone_trainer_5_1-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'rutu_capstone_trainer_5_1-0.1' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python ./setup.py sdist --formats=gztar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ebc029aa-88fc-4f86-9671-38ebb2ad72de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://dist/rutu_capstone_trainer_5_1-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  3.5 KiB/  3.5 KiB]                                                \n",
      "Operation completed over 1 objects/3.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "BUCKET1=rutu-bucket\n",
    "gsutil cp dist/rutu_capstone_trainer_5_1-0.1.tar.gz gs://${BUCKET1}/r-test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7ca09-0e5d-4af8-8ca4-5da49ead27d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud ai custom-jobs describe projects/146339387885/locations/us-central1/customJobs/8920083161765904384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8172-322e-4dee-a582-67a789f2316a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud ai custom-jobs stream-logs projects/146339387885/locations/us-central1/customJobs/8920083161765904384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868723b8-deae-4ceb-a293-0c347df92c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud ai custom-jobs stream-logs projects/146339387885/locations/us-central1/customJobs/5353795206841892864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2ce797-fc66-41e5-8909-b26b992c19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil ls gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/j-test/j-test/jay_capstone2_trainer-0.1.tar.gz\n",
    "#!gsutil ls gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/j-test/jay_capstone2_trainer-0.1.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b7c70db-c01d-464b-806d-5dfaf3adf78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/146339387885/locations/us-central1/customJobs/8546517489159241728] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/146339387885/locations/us-central1/customJobs/8546517489159241728\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/146339387885/locations/us-central1/customJobs/8546517489159241728\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUCKET=rutu-bucket\n",
    "#BUCKET1='qwiklabs-asl-04-76d2f9d7a5ae-jay'\n",
    "\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/r-test/trained_model_$TIMESTAMP\n",
    "JOB_NAME=rutu_ct_new_$TIMESTAMP\n",
    "\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/r-test/rutu_capstone_trainer_5_1-0.1.tar.gz\n",
    "#PYTHON_PACKAGE_URI='gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/j-test/jay_capstone_trainer_5_1-0.1.tar.gz'\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-highmem-16\n",
    "    acceleratorType: NVIDIA_TESLA_P100\n",
    "    acceleratorCount: 2\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/train_3d.csv \n",
    "    - --eval_data_path=gs://${BUCKET}/valid_3d.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8889e5-cf1b-4638-a2e0-15a646ca615b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud ai custom-jobs describe projects/146339387885/locations/us-central1/customJobs/3552359753940205568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85ae806c-92d7-48e7-9a72-06aaf2d5bb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud ai custom-jobs stream-logs projects/146339387885/locations/us-central1/customJobs/3552359753940205568\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477d06f-9f3b-4761-a747-c09222b2cf92",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "To do hyperparameter tuning, create a YAML file and and pass its name with --config. This step could take hours -- you can increase --parallel-trial-count or reduce --max-trial-count to get it done faster. Since --parallel-trial-count is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aae80f67-f83c-4a10-bd4f-faba687103ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [192674431921815552] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 192674431921815552 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUCKET=capstone-datasets\n",
    "BUCKET1='qwiklabs-asl-04-76d2f9d7a5ae-jay'\n",
    "REGION='us-central1'\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "#BASE_OUTPUT_DIR=gs://${BUCKET}/babyweight/hp_tuning_$TIMESTAMP\n",
    "BASE_OUTPUT_DIR=gs://${BUCKET1}/jay-test/trained_model_$TIMESTAMP\n",
    "JOB_NAME=jay_ct_hpt_$TIMESTAMP\n",
    "\n",
    "#PYTHON_PACKAGE_URI=gs://${BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_URI='gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/j-test/jay_capstone_trainer_5_1-0.1.tar.gz'\n",
    "#PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./hyperparam.yaml \"displayName: $JOB_NAME\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: val_loss\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: dropout_rate\n",
    "    doubleValueSpec:\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.4\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  - parameterId: l2_regularization_lambda\n",
    "    doubleValueSpec:\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.4\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  baseOutputDirectory:\n",
    "    outputUriPrefix: $BASE_OUTPUT_DIR\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-highmem-16\n",
    "      acceleratorType: NVIDIA_TESLA_P100\n",
    "      acceleratorCount: 2\n",
    "    pythonPackageSpec:\n",
    "      executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "      packageUris:\n",
    "      - $PYTHON_PACKAGE_URI\n",
    "      pythonModule: $PYTHON_MODULE\n",
    "      args:\n",
    "      - --train_data_path=gs://${BUCKET}/train_3d.csv \n",
    "      - --eval_data_path=gs://${BUCKET}/valid_3d.csv \n",
    "      - --num_epochs=40\n",
    "    replicaCount: 1\"\n",
    "        \n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --max-trial-count=20 \\\n",
    "    --parallel-trial-count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a398914-3c02-4b40-a17c-9508ce38ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2023-04-04T06:27:41.452284Z'\n",
      "displayName: jay_ct_new_20230404_062740\n",
      "endTime: '2023-04-04T06:35:02Z'\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      acceleratorCount: 2\n",
      "      acceleratorType: NVIDIA_TESLA_P100\n",
      "      machineType: n1-highmem-16\n",
      "    pythonPackageSpec:\n",
      "      args:\n",
      "      - --train_data_path=gs://capstone-datasets/train_3d.csv\n",
      "      - --eval_data_path=gs://capstone-datasets/valid_3d.csv\n",
      "      - --output_dir=gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/jay-test/trained_model_20230404_062740\n",
      "      - --num_epochs=10\n",
      "      executorImageUri: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest\n",
      "      packageUris:\n",
      "      - gs://qwiklabs-asl-04-76d2f9d7a5ae-jay/j-test/jay_capstone_trainer_5_1-0.1.tar.gz\n",
      "      pythonModule: trainer.task\n",
      "    replicaCount: '1'\n",
      "name: projects/146339387885/locations/us-central1/customJobs/3552359753940205568\n",
      "startTime: '2023-04-04T06:31:31Z'\n",
      "state: JOB_STATE_SUCCEEDED\n",
      "updateTime: '2023-04-04T06:35:05.128731Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs describe projects/146339387885/locations/us-central1/customJobs/3552359753940205568"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bca6c2-5ac1-4978-a9e0-302ebd5618ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Repeat training\n",
    "This time with tuned parameters for batch_size and nembeds. Note that your best results may differ from below. So be sure to fill yours in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c06fe-36d7-4888-8b29-970d598be996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/babyweight/tuned_$TIMESTAMP\n",
    "JOB_NAME=babyweight_tuned_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./tuned_config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv\n",
    "    - --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=20000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\n",
    "    - --nembeds=8\"\n",
    "    \n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=tuned_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5eee3d4-8d58-4dc0-aae3-4d3b69072ba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convolutional_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_28179/3155636939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolutional_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convolutional_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = convolutional_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae398cf2-c003-43a1-9c6a-539cd80c846d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m104"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
